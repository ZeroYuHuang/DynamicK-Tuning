{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the gradient conflict of Llama model and its K-neuron variants\n",
    "\n",
    " - gradient conflict\n",
    " - dominating gradient\n",
    "\n",
    "目前发现使用topk neuron可能会让模型的gradient conflict增加（无论如何调整gradeint 的norm以及cos相似度大小阈值）\n",
    "\n",
    "**Further analysis**\n",
    " - [ ] Analyze layer by layer, starting from the last layer first\n",
    " - [ ] Analyze neuron by neuron, starting from the neuron that receives max activation values (这个和考虑梯度norm的应该是很类似的，因为接收到更大激活值的neuron，按理来说应该有更大的梯度)\n",
    " - [ ] Analyze weight by weight, 目前是以neuron为单位进行分析的，去看一下 gradient surgery 那篇文章是以什么为单位进行讨论的\n",
    " - [ ] Analyze batch by batch, 目前是以sample为单位进行分析的，不过感觉batch_size=1也是一种情况，不应该被特殊处理，即batch_size为多少都应该\n",
    " - [ ] **第一优先级** 有一种可能是topk的选取方式让neuron在不同sample上获取了更加平均的梯度，即梯度不会被一个sample所dominate。因此即使梯度有冲突也可以很好的average之后进行很好的优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
      "/home/hkustadmin/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/hkustadmin/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TopKLlamaForCausalLM(\n",
       "  (model): TopKLlamadModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-30): 31 x TopKLlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (31): TopKLlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): TopKLlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import random \n",
    "import torch.nn as nn\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from topk_models import TopKLlamaForCausalLM, TopKLlamaConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Configurations of this notobook\n",
    "SEED=1\n",
    "MODEL_PATH = 'models/Llama-2-7b-hf'\n",
    "MODEL_TYPE = 'topk'\n",
    "DATA_PATH = './data/alpaca/'\n",
    "N_SAMPLES = 50\n",
    "CACHE_PATH = 'cache/original_llama/'\n",
    "USE_GATE, USE_UP, USE_DOWN = True, True, True\n",
    "if MODEL_TYPE == 'topk':\n",
    "    from datetime import datetime\n",
    "    currentTime = datetime.now().strftime(\"%H-%M-%S\")\n",
    "    CACHE_PATH = f'cache/{currentTime}'\n",
    "    \n",
    "    os.system(f\"mkdir {CACHE_PATH} & cp topk_7b_configs.json {CACHE_PATH}\")\n",
    "    \n",
    "# load the model\n",
    "if MODEL_TYPE == 'original':\n",
    "    model = LlamaForCausalLM.from_pretrained(\"./models/Llama-2-7b-hf/\", device_map=\"auto\")\n",
    "else:\n",
    "    # load the topk model\n",
    "    config = TopKLlamaConfig.from_json_file('topk_7b_configs.json')\n",
    "    model = TopKLlamaForCausalLM.from_pretrained(\n",
    "        \"./models/Llama-2-7b-hf/\", \n",
    "        # device_map=\"auto\",\n",
    "        config=config\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/Llama-2-7b-hf/\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample some data points from the instruction-tuning data, e.g., Alpaca52K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "random.seed(10)\n",
    "data = load_dataset(DATA_PATH)['train']\n",
    "data_idx_sample = random.sample([_ for _ in range(len(data))], N_SAMPLES)\n",
    "data_samples = [data[i] for i in data_idx_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register gradients of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:05<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from utils import PROMPT_TEMPLATE_SINGLE as PROMPT_DICT\n",
    "from torch import optim\n",
    "prompt = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "\n",
    "n_layer = 32\n",
    "grads = {\n",
    "    \"gate_proj\": [[] for _ in range(n_layer)], \"up_proj\": [[] for _ in range(n_layer)], \"down_proj\": [[] for _ in range(n_layer)]\n",
    "}\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "for d in tqdm(data_samples):\n",
    "    inp = prompt.format_map(d)\n",
    "    out = f\"{d['output']}{tokenizer.eos_token}\"\n",
    "    text = inp + out\n",
    "    # tokenize\n",
    "    input_ids = tokenizer(text, return_tensors='pt').input_ids# .to(device)\n",
    "    src_len = tokenizer(inp, return_tensors='pt').input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "    labels = deepcopy(input_ids)\n",
    "    labels[:, : src_len] = IGNORE_INDEX\n",
    "    labels = labels[:, 1:]\n",
    "    \n",
    "    # forward\n",
    "    y = model(input_ids[:,:-1])\n",
    "    loss = F.cross_entropy(input=y.logits.squeeze(), target=labels.squeeze())\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # record gradients\n",
    "    for n, p in model.named_parameters():\n",
    "        if len(n.split(\".\")) < 5:\n",
    "            continue\n",
    "        layer_id = int(n.split(\".\")[2])\n",
    "        name = n.split(\".\")[4]\n",
    "        if name in grads:\n",
    "            grads[name][layer_id].append(p.grad.data.detach().cpu())\n",
    "            \n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the gradient and analyze their gradient conflicting, for now let's just focus on one neuron in the FFN layer.\n",
    "\n",
    "The representation of neuron $i$ in Llama FFN layer could be defined as $[g_i, up_i, down_i]$, where $g_i$, $up_i$, $down_i$ represent the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [00:10<05:16, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7166)\n",
      "tensor(0.1922)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/32 [00:19<04:55,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7225)\n",
      "tensor(0.1304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/32 [00:29<04:45,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7253)\n",
      "tensor(0.1131)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:39<04:39,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7289)\n",
      "tensor(0.1379)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/32 [00:49<04:30, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7293)\n",
      "tensor(0.1652)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/32 [01:00<04:21, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7332)\n",
      "tensor(0.1689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 7/32 [01:10<04:13, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7343)\n",
      "tensor(0.1635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 8/32 [01:20<04:04, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7376)\n",
      "tensor(0.1742)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 9/32 [01:31<03:55, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7418)\n",
      "tensor(0.1893)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [01:41<03:45, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7424)\n",
      "tensor(0.1929)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 11/32 [01:51<03:37, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7459)\n",
      "tensor(0.2024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 12/32 [02:02<03:28, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7470)\n",
      "tensor(0.2041)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 13/32 [02:13<03:18, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7477)\n",
      "tensor(0.2080)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 14/32 [02:23<03:08, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7494)\n",
      "tensor(0.2069)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 15/32 [02:33<02:57, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7507)\n",
      "tensor(0.1970)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 16/32 [02:44<02:47, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7515)\n",
      "tensor(0.1828)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 17/32 [02:55<02:38, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7477)\n",
      "tensor(0.1575)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 18/32 [03:05<02:26, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7471)\n",
      "tensor(0.1426)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 19/32 [03:16<02:16, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7432)\n",
      "tensor(0.1314)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [03:26<02:05, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7397)\n",
      "tensor(0.1273)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 21/32 [03:36<01:54, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7316)\n",
      "tensor(0.1131)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 22/32 [03:47<01:44, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7283)\n",
      "tensor(0.1055)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 23/32 [03:57<01:34, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7255)\n",
      "tensor(0.0998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 24/32 [04:08<01:23, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7238)\n",
      "tensor(0.0976)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 25/32 [04:18<01:13, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7233)\n",
      "tensor(0.0989)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 26/32 [04:29<01:03, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7230)\n",
      "tensor(0.1030)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 27/32 [04:40<00:53, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7255)\n",
      "tensor(0.1076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 28/32 [04:50<00:42, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7300)\n",
      "tensor(0.1192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 29/32 [05:02<00:32, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7383)\n",
      "tensor(0.1288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [05:13<00:21, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7460)\n",
      "tensor(0.1383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [05:23<00:10, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7540)\n",
      "tensor(0.1541)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [05:34<00:00, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan)\n",
      "tensor(0.3437)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_layers = model.config.num_hidden_layers\n",
    "# for the ffn of each layer\n",
    "# (neuron_num, sample, d_dim) @ (neuron_num, d_dim, sample)\n",
    "norm_dominate = []\n",
    "conflicts = []\n",
    "epsilon = 0\n",
    "gamma = 0\n",
    "device='cpu'\n",
    "from tqdm import trange\n",
    "for layer_id in trange(n_layers):\n",
    "    gate_grads = torch.stack(grads[\"gate_proj\"][layer_id]).transpose(0, 1)\n",
    "    up_grads = torch.stack(grads[\"up_proj\"][layer_id]).transpose(0, 1)\n",
    "    down_grads = torch.stack(grads[\"down_proj\"][layer_id]).permute(2, 0, 1)\n",
    "    neuron_grads = torch.cat([\n",
    "        gate_grads if USE_GATE else torch.zeros_like(gate_grads), \n",
    "        up_grads if USE_UP else torch.zeros_like(up_grads), \n",
    "        down_grads if USE_DOWN else torch.zeros_like(down_grads)], dim=-1).to(device)\n",
    "    \n",
    "    grad_norm = torch.norm(neuron_grads, p=2, dim=-1, keepdim=True)\n",
    "    grad_norm_ = torch.cat([grad_norm for _ in range(N_SAMPLES)], dim=-1)\n",
    "    grad_norm_2 = grad_norm_ * grad_norm_\n",
    "    tmp = torch.bmm(grad_norm, grad_norm.transpose(1, 2))\n",
    "    gm_sim = 2 * torch.bmm(grad_norm, grad_norm.transpose(1, 2)) / (grad_norm_2 + grad_norm_2.transpose(1,2))\n",
    "    norm_dominate.append(torch.mean(gm_sim).cpu().data)\n",
    "    \n",
    "    norm_mask = grad_norm > epsilon\n",
    "    # print(torch.sum(norm_mask) / N_SAMPLES / 11008)\n",
    "    norm_mask = norm_mask.float()\n",
    "    batch_norm_mask = torch.bmm(norm_mask, norm_mask.transpose(1, 2)).cpu()\n",
    "    \n",
    "    _a = F.normalize(neuron_grads, p=2, dim=-1)\n",
    "    _b = _a.transpose(1, 2)\n",
    "    conflict = torch.bmm(_a, _b).cpu()\n",
    "    conflict.masked_fill_(~batch_norm_mask.bool(), 0)\n",
    "    conflict_rate = torch.sum(conflict < gamma) / torch.sum(batch_norm_mask)\n",
    "    conflicts.append(conflict_rate.data)\n",
    "    print(torch.mean(gm_sim))\n",
    "    print(conflicts[-1])\n",
    "    \n",
    "# plt.bar([ _ for _ in range(2)], conflicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘cache/17-27-34’: File exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.system(f\"mkdir {CACHE_PATH}\")\n",
    "np.save(f'{CACHE_PATH}/conflict_{SEED}_{epsilon}_{gamma}.npy', conflicts)\n",
    "np.save(f'{CACHE_PATH}/gm_sim_{SEED}_{epsilon}_{gamma}.npy', norm_dominate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cache/17-27-34/conflict_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hkustadmin/huangzeyu/DynamicK-Tuning/analysis_grads.ipynb 单元格 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.96.164.59/home/hkustadmin/huangzeyu/DynamicK-Tuning/analysis_grads.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.96.164.59/home/hkustadmin/huangzeyu/DynamicK-Tuning/analysis_grads.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mCACHE_PATH\u001b[39m}\u001b[39;49;00m\u001b[39m/conflict_\u001b[39;49m\u001b[39m{\u001b[39;49;00mepsilon\u001b[39m}\u001b[39;49;00m\u001b[39m.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.96.164.59/home/hkustadmin/huangzeyu/DynamicK-Tuning/analysis_grads.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m res2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcache/19-46-26/conflict_0.01.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.96.164.59/home/hkustadmin/huangzeyu/DynamicK-Tuning/analysis_grads.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m x \u001b[39m=\u001b[39m [ _ \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers)]\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cache/17-27-34/conflict_0.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "res = np.load(f'{CACHE_PATH}/conflict_{epsilon}.npy')\n",
    "res2 = np.load(f'cache/19-46-26/conflict_0.01.npy')\n",
    "x = [ _ for _ in range(n_layers)]\n",
    "plt.plot(x, res)\n",
    "plt.plot(x, res2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
